2025-12-25 02:30:49,041 [INFO] main.py:10 - Started
2025-12-25 02:30:49,041 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 02:30:49,041 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 02:30:49,042 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 02:30:49,042 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 02:30:49,042 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 02:30:49,042 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 02:30:49,042 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 02:30:49,042 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 02:30:49,042 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 02:30:49,088 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 02:30:49,109 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 02:30:49,134 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 02:30:49,167 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 02:30:49,167 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 02:30:49,173 [INFO] base.py:214 - Scheduler started
2025-12-25 02:30:49,173 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 02:30:49,240 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 02:30:49,240 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 02:30:49,240 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 02:31:09,794 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 02:31:09,799 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 02:31:09,802 [DEBUG] _trace.py:87 - connect_tcp.failed exception=ConnectError(OSError('All connection attempts failed'))
2025-12-25 02:31:09,803 [DEBUG] _trace.py:87 - connect_tcp.failed exception=ConnectError(OSError('All connection attempts failed'))
2025-12-25 02:31:11,866 [DEBUG] _trace.py:87 - connect_tcp.started host='172.23.1.3' port=8080 local_address=None timeout=2.0 socket_options=None
2025-12-25 02:31:12,048 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f8ab770>
2025-12-25 02:31:12,049 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 02:31:12,050 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 02:31:12,051 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 02:31:12,051 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 02:31:12,052 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 02:31:12,057 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Server', b'llama.cpp'), (b'Content-Length', b'594'), (b'Access-Control-Allow-Origin', b'')])
2025-12-25 02:31:12,059 [INFO] _client.py:1740 - HTTP Request: GET http://172.23.1.3:8080/v1/models "HTTP/1.1 200 OK"
2025-12-25 02:31:12,060 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 02:31:12,060 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 02:31:12,061 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 02:31:12,061 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 02:31:12,062 [DEBUG] _trace.py:87 - close.started
2025-12-25 02:31:12,062 [DEBUG] _trace.py:87 - close.complete
2025-12-25 02:31:35,492 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b56cec3f-c9af-465d-987f-928fd0709527', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'bartowski/openai_gpt-oss-20b-GGUF', 'stream': True, 'temperature': 0.7}}
2025-12-25 02:31:35,493 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://172.23.1.3:8080/v1/chat/completions
2025-12-25 02:31:35,493 [DEBUG] _trace.py:87 - connect_tcp.started host='172.23.1.3' port=8080 local_address=None timeout=None socket_options=None
2025-12-25 02:31:35,583 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x110038550>
2025-12-25 02:31:35,583 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 02:31:35,584 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 02:31:35,584 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 02:31:35,584 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 02:31:35,584 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 02:31:37,834 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
2025-12-25 02:31:37,835 [INFO] _client.py:1740 - HTTP Request: POST http://172.23.1.3:8080/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 02:31:37,835 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://172.23.1.3:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
2025-12-25 02:31:37,835 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 02:31:37,836 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 02:31:39,748 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 02:31:39,749 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 02:31:39,749 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 02:34:21,130 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e443a4e4-706a-4353-baee-91f106f21cc3', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'How are you?', 'role': 'user'}], 'model': 'bartowski/openai_gpt-oss-20b-GGUF', 'stream': True, 'temperature': 0.7}}
2025-12-25 02:34:21,131 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://172.23.1.3:8080/v1/chat/completions
2025-12-25 02:34:21,131 [DEBUG] _trace.py:87 - connect_tcp.started host='172.23.1.3' port=8080 local_address=None timeout=None socket_options=None
2025-12-25 02:34:21,170 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11003afd0>
2025-12-25 02:34:21,170 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 02:34:21,170 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 02:34:21,170 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 02:34:21,170 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 02:34:21,170 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 02:34:21,876 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
2025-12-25 02:34:21,877 [INFO] _client.py:1740 - HTTP Request: POST http://172.23.1.3:8080/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 02:34:21,878 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://172.23.1.3:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
2025-12-25 02:34:21,878 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 02:34:21,878 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 02:34:24,033 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 02:34:24,034 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 02:34:24,034 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 02:41:42,842 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3fe80f49-f841-4cc4-a284-de1ce1842a31', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'How are you?', 'role': 'user'}, {'content': 'I‚Äôm doing great‚Äîthanks for asking! How about you? Anything on your mind today?', 'role': 'assistant'}, {'content': 'Yes, I want to start painting', 'role': 'user'}], 'model': 'bartowski/openai_gpt-oss-20b-GGUF', 'stream': True, 'temperature': 0.7}}
2025-12-25 02:41:42,847 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://172.23.1.3:8080/v1/chat/completions
2025-12-25 02:41:42,849 [DEBUG] _trace.py:87 - connect_tcp.started host='172.23.1.3' port=8080 local_address=None timeout=None socket_options=None
2025-12-25 02:41:42,930 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11000e060>
2025-12-25 02:41:42,931 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 02:41:42,933 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 02:41:42,934 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 02:41:42,935 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 02:41:42,935 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 02:41:43,985 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Keep-Alive', b'timeout=5, max=100'), (b'Content-Type', b'text/event-stream'), (b'Server', b'llama.cpp'), (b'Transfer-Encoding', b'chunked'), (b'Access-Control-Allow-Origin', b'')])
2025-12-25 02:41:43,990 [INFO] _client.py:1740 - HTTP Request: POST http://172.23.1.3:8080/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 02:41:43,991 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://172.23.1.3:8080/v1/chat/completions "200 OK" Headers({'keep-alive': 'timeout=5, max=100', 'content-type': 'text/event-stream', 'server': 'llama.cpp', 'transfer-encoding': 'chunked', 'access-control-allow-origin': ''})
2025-12-25 02:41:43,992 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 02:41:43,994 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 02:43:02,902 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 02:43:02,904 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 02:43:02,905 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 11:14:24,662 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 11:14:24,662 [INFO] main.py:10 - Started
2025-12-25 11:14:24,662 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 11:14:24,662 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 11:14:24,662 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 11:14:24,663 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 11:14:24,663 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 11:14:24,663 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 11:14:24,663 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 11:14:24,663 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 11:14:24,663 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 11:14:24,671 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 11:14:24,691 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 11:14:24,714 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 11:14:24,744 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 11:14:24,744 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 11:14:24,749 [INFO] base.py:214 - Scheduler started
2025-12-25 11:14:24,749 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 11:14:24,787 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 11:14:24,787 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 11:14:24,788 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 11:14:49,282 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:14:49,288 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:14:49,289 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11357c050>
2025-12-25 11:14:49,289 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 11:14:49,289 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:14:49,289 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 11:14:49,289 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:14:49,289 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 11:14:49,290 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1134d3610>
2025-12-25 11:14:49,290 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 11:14:49,290 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:14:49,290 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 11:14:49,290 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:14:49,290 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 11:14:49,292 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 08:14:49 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 11:14:49,293 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 11:14:49,293 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 11:14:49,293 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 11:14:49,293 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:14:49,293 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:14:49,293 [DEBUG] _trace.py:87 - close.started
2025-12-25 11:14:49,293 [DEBUG] _trace.py:87 - close.complete
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 08:14:49 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 11:14:49,294 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - close.started
2025-12-25 11:14:49,294 [DEBUG] _trace.py:87 - close.complete
2025-12-25 11:14:57,093 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-26cd37b7-fed6-4ac8-a7e3-9ba17e866845', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 11:14:57,094 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 11:14:57,094 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 11:14:57,096 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113b76c10>
2025-12-25 11:14:57,096 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 11:14:57,097 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:14:57,097 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 11:14:57,097 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:14:57,097 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 11:14:59,602 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 08:14:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 11:14:59,603 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 11:14:59,604 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 08:14:59 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 11:14:59,604 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 11:14:59,604 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 11:15:00,000 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:15:00,001 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:15:00,007 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 11:16:14,004 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b7731b63-2977-4064-b698-f102607c8f9c', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi there! How can I assist you today? üòä', 'role': 'assistant'}, {'content': 'I am located at Istanbul. Today is 25-DEC. How is the weather conditions during these days historically?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 11:16:14,005 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 11:16:14,005 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 11:16:14,007 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113bddcd0>
2025-12-25 11:16:14,007 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 11:16:14,007 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:16:14,007 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 11:16:14,007 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:16:14,007 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 11:16:14,011 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 08:16:14 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 11:16:14,011 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 11:16:14,011 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 08:16:14 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 11:16:14,011 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 11:16:14,011 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 11:16:26,769 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:16:26,769 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:16:26,775 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 11:57:12,455 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:57:12,462 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:57:12,465 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113bdf820>
2025-12-25 11:57:12,466 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 11:57:12,466 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:57:12,466 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113acbd10>
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:57:12,467 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 11:57:12,483 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-KAdhwrfkam43DZw6QP77cWXlm4c"'), (b'Date', b'Thu, 25 Dec 2025 08:57:12 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 11:57:12,484 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 11:57:12,484 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 11:57:12,484 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 11:57:12,484 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:57:12,484 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:57:12,484 [DEBUG] _trace.py:87 - close.started
2025-12-25 11:57:12,485 [DEBUG] _trace.py:87 - close.complete
2025-12-25 11:57:12,485 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-KAdhwrfkam43DZw6QP77cWXlm4c"'), (b'Date', b'Thu, 25 Dec 2025 08:57:12 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 11:57:12,485 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 11:57:12,485 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 11:57:12,485 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 11:57:12,486 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:57:12,486 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:57:12,486 [DEBUG] _trace.py:87 - close.started
2025-12-25 11:57:12,486 [DEBUG] _trace.py:87 - close.complete
2025-12-25 11:57:14,138 [DEBUG] _trace.py:87 - connect_tcp.started host='172.23.1.3' port=8080 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:57:16,140 [DEBUG] _trace.py:87 - connect_tcp.failed exception=ConnectTimeout(TimeoutError())
2025-12-25 11:57:38,130 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:57:38,131 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113b9bdf0>
2025-12-25 11:57:38,132 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 11:57:38,132 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:57:38,132 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 11:57:38,132 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:57:38,132 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-KAdhwrfkam43DZw6QP77cWXlm4c"'), (b'Date', b'Thu, 25 Dec 2025 08:57:38 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 11:57:38,134 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - close.started
2025-12-25 11:57:38,134 [DEBUG] _trace.py:87 - close.complete
2025-12-25 11:57:39,583 [DEBUG] _trace.py:87 - connect_tcp.started host='127.0.0.1' port=8080 local_address=None timeout=2.0 socket_options=None
2025-12-25 11:57:39,584 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113c30e20>
2025-12-25 11:57:39,584 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 11:57:39,585 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:57:39,585 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 11:57:39,585 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:57:39,585 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 11:57:39,588 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Keep-Alive', b'timeout=5, max=100'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Content-Length', b'594')])
2025-12-25 11:57:39,588 [INFO] _client.py:1740 - HTTP Request: GET http://127.0.0.1:8080/v1/models "HTTP/1.1 200 OK"
2025-12-25 11:57:39,588 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 11:57:39,588 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 11:57:39,589 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:57:39,589 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:57:39,589 [DEBUG] _trace.py:87 - close.started
2025-12-25 11:57:39,589 [DEBUG] _trace.py:87 - close.complete
2025-12-25 11:57:42,914 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-0b20586d-d827-4586-8713-c492f962a2a2', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'bartowski/openai_gpt-oss-20b-GGUF', 'stream': True, 'temperature': 0.7}}
2025-12-25 11:57:42,914 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://127.0.0.1:8080/v1/chat/completions
2025-12-25 11:57:42,914 [DEBUG] _trace.py:87 - connect_tcp.started host='127.0.0.1' port=8080 local_address=None timeout=None socket_options=None
2025-12-25 11:57:42,915 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113c82050>
2025-12-25 11:57:42,915 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 11:57:42,915 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 11:57:42,915 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 11:57:42,916 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 11:57:42,916 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 11:57:43,426 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'text/event-stream'), (b'Keep-Alive', b'timeout=5, max=100'), (b'Server', b'llama.cpp'), (b'Access-Control-Allow-Origin', b''), (b'Transfer-Encoding', b'chunked')])
2025-12-25 11:57:43,427 [INFO] _client.py:1740 - HTTP Request: POST http://127.0.0.1:8080/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 11:57:43,427 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://127.0.0.1:8080/v1/chat/completions "200 OK" Headers({'content-type': 'text/event-stream', 'keep-alive': 'timeout=5, max=100', 'server': 'llama.cpp', 'access-control-allow-origin': '', 'transfer-encoding': 'chunked'})
2025-12-25 11:57:43,427 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 11:57:43,427 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 11:57:44,026 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 11:57:44,026 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 11:57:44,030 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:11:46,802 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:11:46,803 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113c81850>
2025-12-25 12:11:46,804 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:11:46,804 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:11:46,804 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:11:46,804 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:11:46,804 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:11:46 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:11:46,806 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:11:46,806 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:11:58,236 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-268d5315-8bbc-4ee1-922b-a95a5646750d', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'Whatz up man?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:11:58,237 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:11:58,237 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:11:58,238 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113c3cc80>
2025-12-25 12:11:58,238 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:11:58,238 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:11:58,238 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:11:58,239 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:11:58,239 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:11:58,242 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:11:58 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:11:58,242 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:11:58,242 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:11:58 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:11:58,242 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:11:58,242 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:11:59,450 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:11:59,450 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:11:59,452 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:12:26,965 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1ee3aec7-2ffe-4f3b-837f-d44426df72ed', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'Whatz up man?', 'role': 'user'}, {'content': 'Haha, got it‚Äî**what‚Äôs good?** ‚òÄÔ∏èüòé\n\nHow‚Äôs *your* day treating you? Need help with anything‚Äîcoding, advice, random thoughts, or just vibing? Drop it and let‚Äôs chat! üëä', 'role': 'assistant'}, {'content': 'Yeah, coding bitch', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:12:26,965 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:12:26,965 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:12:26,967 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11343f3e0>
2025-12-25 12:12:26,967 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:12:26,967 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:12:26,967 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:12:26,967 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:12:26,967 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:12:26,970 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:12:26 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:12:26,970 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:12:26,970 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:12:26 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:12:26,970 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:12:26,970 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:12:28,820 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:12:28,821 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:12:28,823 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:13:50,126 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-04dc471b-040b-407b-ade5-7e8d8e050d04', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'Whatz up man?', 'role': 'user'}, {'content': 'Haha, got it‚Äî**what‚Äôs good?** ‚òÄÔ∏èüòé\n\nHow‚Äôs *your* day treating you? Need help with anything‚Äîcoding, advice, random thoughts, or just vibing? Drop it and let‚Äôs chat! üëä', 'role': 'assistant'}, {'content': 'Yeah, coding bitch', 'role': 'user'}, {'content': 'Haha, **"bitch"** for speed! üöÄ Coding mode: **ON**\n\nGot a project in mind?\n- Need help debugging something?\n- Looking for code snippets or algorithms?\n- Just here to geek out about memes/coding humor? (I‚Äôll respect either! üòÇ)\n\nAsk away‚Äîwhat‚Äôs the *real* question? üíªüî•', 'role': 'assistant'}, {'content': 'Ok time to be serious. Thanks for handling my previous messages kindly :)\nI would like to create a C algorithm for quick sort. How can I start?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:13:50,127 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:13:50,127 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:13:50,128 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113bed1d0>
2025-12-25 12:13:50,128 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:13:50,129 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:13:50,129 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:13:50,129 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:13:50,129 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:13:50,133 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:13:50 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:13:50,133 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:13:50,133 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:13:50 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:13:50,133 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:13:50,133 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:14:05,437 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:14:05,437 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:14:05,443 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:14:59,909 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ce856a81-b0f4-41aa-89e1-334236f822bb', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'Whatz up man?', 'role': 'user'}, {'content': 'Haha, got it‚Äî**what‚Äôs good?** ‚òÄÔ∏èüòé\n\nHow‚Äôs *your* day treating you? Need help with anything‚Äîcoding, advice, random thoughts, or just vibing? Drop it and let‚Äôs chat! üëä', 'role': 'assistant'}, {'content': 'Yeah, coding bitch', 'role': 'user'}, {'content': 'Haha, **"bitch"** for speed! üöÄ Coding mode: **ON**\n\nGot a project in mind?\n- Need help debugging something?\n- Looking for code snippets or algorithms?\n- Just here to geek out about memes/coding humor? (I‚Äôll respect either! üòÇ)\n\nAsk away‚Äîwhat‚Äôs the *real* question? üíªüî•', 'role': 'assistant'}, {'content': 'Ok time to be serious. Thanks for handling my previous messages kindly :)\nI would like to create a C algorithm for quick sort. How can I start?', 'role': 'user'}, {'content': 'No problem! QuickSort is a classic **divide-and-conquer** algorithm that sorts arrays by partitioning them around a pivot element.\n\n### **Steps to Implement QuickSort in C**\nHere‚Äôs how you can start:\n\n#### **1. Understand the Logic**\n- Choose a pivot (e.g., last element).\n- Partition the array into two subarrays:\n  - Elements ‚â§ pivot (left)\n  - Elements > pivot (right)\n- Recursively sort the left and right partitions.\n\n#### **2. Basic QuickSort Implementation**\n```c\n#include <stdio.h>\n\n// Function to partition the array\nint partition(int arr int low, int high) {\n    int pivot = arrhigh // Choose last element as pivot\n    int i = (low - 1);     // Index of smaller element\n\n    for (int j = low; j <= high - 1; j++) {\n        if (arr <= pivot) { // If current element is ‚â§ pivot\n            i++;             // Move index forward\n            // Swap elements at i and j\n            int temp = arr            arr = arr            arr = temp;\n        }\n    }\n    // Place pivot in correct position\n    int temp2 = arr + 1    arr + 1 = arrhigh    arrhigh = temp2;\n\n    return (i + 1); // Return the partition index\n}\n\n// QuickSort function\nvoid quickSort(int arr int low, int high) {\n    if (low < high) {       // Base case: array has more than one element\n        int pi = partition(arr, low, high); // Partitioning index\n\n        // Recursively sort elements before and after partition\n        quickSort(arr, low, pi - 1);\n        quickSort(arr, pi + 1, high);\n    }\n}\n\n// Helper function to print an array\nvoid printArray(int arr int size) {\n    for (int i = 0; i < size; i++)\n        printf("%d ", arr    printf("\\n");\n}\n\n// Main function (driver code)\nint main() {\n    int arr = {10, 7, 8, 9, 1, 5};\n    int n = sizeof(arr) / sizeof(arr0    printf("Original array: ");\n    printArray(arr, n);\n\n    quickSort(arr, 0, n - 1); // Sort the array\n\n    printf("Sorted array:   ");\n    printArray(arr, n);\n    return 0;\n}\n```\n\n#### **3. Key Notes**\n‚úÖ **Partitioning**: Critical step‚Äîchoose pivot wisely (first/last/random).\n‚úÖ **Recursion**: QuickSort is recursive‚Äîbase case must exist (`low < high`).\n‚úÖ **Time Complexity**:\n   - **Best/Average Case**: O(n log n)\n   - **Worst Case** (if pivot is smallest/largest): O(n¬≤) ‚Üí Use random pivot for robustness.\n\n#### **4. Try It Yourself**\n- Compile & run the code.\n- Modify `arr` to test different inputs.\n- Experiment with edge cases (empty array, single-element array).\n\nWant to dive deeper? Let me know:\n1. What part confuses you?\n2. Should we optimize further? (e.g., tail recursion, randomized pivot)\n3. Any other questions? üòä\n\n*(Also, if you‚Äôre learning, consider comparing it with MergeSort‚Äîboth are great but behave differently!)*ng it with MergeSort‚Äîboth are great but behave differently!)*', 'role': 'assistant'}, {'content': 'How can I do this in C++?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:14:59,909 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:14:59,909 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:14:59,911 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113bee350>
2025-12-25 12:14:59,911 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:14:59,911 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:14:59,911 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:14:59,911 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:14:59,911 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:14:59,914 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:14:59 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:14:59,914 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:14:59,914 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:14:59 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:14:59,914 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:14:59,914 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:15:20,305 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:15:20,306 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:15:20,312 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:15:25,982 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-72a150ac-6ede-405f-b11d-61b593f66a24', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'Whatz up man?', 'role': 'user'}, {'content': 'Haha, got it‚Äî**what‚Äôs good?** ‚òÄÔ∏èüòé\n\nHow‚Äôs *your* day treating you? Need help with anything‚Äîcoding, advice, random thoughts, or just vibing? Drop it and let‚Äôs chat! üëä', 'role': 'assistant'}, {'content': 'Yeah, coding bitch', 'role': 'user'}, {'content': 'Haha, **"bitch"** for speed! üöÄ Coding mode: **ON**\n\nGot a project in mind?\n- Need help debugging something?\n- Looking for code snippets or algorithms?\n- Just here to geek out about memes/coding humor? (I‚Äôll respect either! üòÇ)\n\nAsk away‚Äîwhat‚Äôs the *real* question? üíªüî•', 'role': 'assistant'}, {'content': 'Ok time to be serious. Thanks for handling my previous messages kindly :)\nI would like to create a C algorithm for quick sort. How can I start?', 'role': 'user'}, {'content': 'No problem! QuickSort is a classic **divide-and-conquer** algorithm that sorts arrays by partitioning them around a pivot element.\n\n### **Steps to Implement QuickSort in C**\nHere‚Äôs how you can start:\n\n#### **1. Understand the Logic**\n- Choose a pivot (e.g., last element).\n- Partition the array into two subarrays:\n  - Elements ‚â§ pivot (left)\n  - Elements > pivot (right)\n- Recursively sort the left and right partitions.\n\n#### **2. Basic QuickSort Implementation**\n```c\n#include <stdio.h>\n\n// Function to partition the array\nint partition(int arr int low, int high) {\n    int pivot = arrhigh // Choose last element as pivot\n    int i = (low - 1);     // Index of smaller element\n\n    for (int j = low; j <= high - 1; j++) {\n        if (arr <= pivot) { // If current element is ‚â§ pivot\n            i++;             // Move index forward\n            // Swap elements at i and j\n            int temp = arr            arr = arr            arr = temp;\n        }\n    }\n    // Place pivot in correct position\n    int temp2 = arr + 1    arr + 1 = arrhigh    arrhigh = temp2;\n\n    return (i + 1); // Return the partition index\n}\n\n// QuickSort function\nvoid quickSort(int arr int low, int high) {\n    if (low < high) {       // Base case: array has more than one element\n        int pi = partition(arr, low, high); // Partitioning index\n\n        // Recursively sort elements before and after partition\n        quickSort(arr, low, pi - 1);\n        quickSort(arr, pi + 1, high);\n    }\n}\n\n// Helper function to print an array\nvoid printArray(int arr int size) {\n    for (int i = 0; i < size; i++)\n        printf("%d ", arr    printf("\\n");\n}\n\n// Main function (driver code)\nint main() {\n    int arr = {10, 7, 8, 9, 1, 5};\n    int n = sizeof(arr) / sizeof(arr0    printf("Original array: ");\n    printArray(arr, n);\n\n    quickSort(arr, 0, n - 1); // Sort the array\n\n    printf("Sorted array:   ");\n    printArray(arr, n);\n    return 0;\n}\n```\n\n#### **3. Key Notes**\n‚úÖ **Partitioning**: Critical step‚Äîchoose pivot wisely (first/last/random).\n‚úÖ **Recursion**: QuickSort is recursive‚Äîbase case must exist (`low < high`).\n‚úÖ **Time Complexity**:\n   - **Best/Average Case**: O(n log n)\n   - **Worst Case** (if pivot is smallest/largest): O(n¬≤) ‚Üí Use random pivot for robustness.\n\n#### **4. Try It Yourself**\n- Compile & run the code.\n- Modify `arr` to test different inputs.\n- Experiment with edge cases (empty array, single-element array).\n\nWant to dive deeper? Let me know:\n1. What part confuses you?\n2. Should we optimize further? (e.g., tail recursion, randomized pivot)\n3. Any other questions? üòä\n\n*(Also, if you‚Äôre learning, consider comparing it with MergeSort‚Äîboth are great but behave differently!)*ng it with MergeSort‚Äîboth are great but behave differently!)*', 'role': 'assistant'}, {'content': 'How can I do this in C++?', 'role': 'user'}, {'content': 'Here‚Äôs how you can implement **QuickSort in C++** using a similar approach to the previous C version:\n\n### **QuickSort in C++**\n```cpp\n#include <iostream>\nusing namespace std;\n\n// Function to partition the array and return the pivot index\nint partition(vector<int>& arr, int low, int high) {\n    int pivot = arrhigh // Choose last element as pivot\n    int i = (low - 1);     // Index of smaller element\n\n    for (int j = low; j <= high - 1; j++) {\n        if (arr <= pivot) { // If current element is ‚â§ pivot\n            i++;              // Move index forward\n            swap(arr arr // Swap elements\n        }\n    }\n\n    // Place pivot in correct position\n    swap(arr + 1 arrhigh    return (i + 1); // Return the partition index\n}\n\n// QuickSort function (recursive)\nvoid quickSort(vector<int>& arr, int low, int high) {\n    if (low < high) {       // Base case: array has more than one element\n        int pi = partition(arr, low, high); // Partitioning index\n\n        // Recursively sort elements before and after partition\n        quickSort(arr, low, pi - 1);\n        quickSort(arr, pi + 1, high);\n    }\n}\n\n// Helper function to print an array (vector)\nvoid printArray(const vector<int>& arr) {\n    for (int num : arr)\n        cout << num << " ";\n    cout << endl;\n}\n\nint main() {\n    vector<int> arr = {10, 7, 8, 9, 1, 5};\n    int n = arr.size();\n\n    cout << "Original array: ";\n    printArray(arr);\n\n    quickSort(arr, 0, n - 1); // Sort the array\n\n    cout << "Sorted array:   ";\n    printArray(arr);\n\n    return 0;\n}\n```\n\n---\n\n### **Key Differences from C Version**\n1. **Using `std::vector` (instead of raw arrays)**\n   - Simplifies dynamic resizing.\n   - Easier to pass around in functions.\n\n2. **Passing by reference (`vector<int>& arr`)**\n   - Avoids copying large arrays unnecessarily.\n\n3. **`swap()` instead of manual swapping**\n   - Cleaner and more efficient way to swap elements.\n\n---\n\n### **Optimizations & Best Practices**\n#### 1. **Random Pivot Selection**\n   To avoid worst-case O(n¬≤) performance (e.g., when the array is already sorted):\n   ```cpp\n   int pivot = arrrand() % (high - low + 1) + low   ```\n\n#### 2. **Tail Recursion Optimization** (optional)\n   Reduces recursion stack usage by sorting the smaller partition first:\n   ```cpp\n   void quickSort(vector<int>& arr, int low, int high) {\n       while (low < high) { // Use a loop instead of recursion for the larger part\n           int pi = partition(arr, low, high);\n           if (pi - low > high - pi) {  // Sort smaller subarray first\n               quickSort(arr, low, pi - 1);\n               low = pi + 1;\n           } else {\n               quickSort(arr, pi + 1, high);\n               high = pi - 1;\n           }\n       }\n   }\n   ```\n\n#### 3. **Hybrid Approach (Insertion Sort for small subarrays)**\n   For small arrays (e.g., size < 10), insertion sort performs better than QuickSort.\n\n---\n\n### **Testing & Debugging**\n- **Test Cases**:\n  - Already sorted array: `{1, 2, 3, 4}`\n  - Reverse-sorted array: `{5, 4, 3, 2, 1}`\n  - Randomly ordered array: `{9, 2, 7, 4, 6}`\n- **Edge Cases**:\n  - Empty array (`{}`).\n  - Single-element array (`{5}`).\n\n---\n\n### **Next Steps**\nWould you like:\n1. A more optimized version (random pivot + tail recursion)?\n2. An explanation of how this compares to MergeSort?\n3. Help with any specific part (e.g., debugging)? üòäic part (e.g., debugging)? üòä', 'role': 'assistant'}, {'content': 'Python version', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:15:25,983 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:15:25,983 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:15:25,984 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113c09230>
2025-12-25 12:15:25,985 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:15:25,985 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:15:25,985 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:15:25,985 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:15:25,985 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:15:25,989 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:15:25 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:15:25,989 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:15:25,989 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:15:25 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:15:25,989 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:15:25,990 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:15:41,222 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:15:41,223 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:15:41,228 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:32:09,854 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:32:09,858 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113b33a10>
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113b32750>
2025-12-25 12:32:09,859 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:32:09,860 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:32:09,860 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:32:09,860 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:32:09,860 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:32:09 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:32:09,861 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:32:09,861 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:32:09 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:32:09,862 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:32:09,862 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:32:41,078 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 12:32:44,453 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 12:32:44,454 [INFO] main.py:10 - Started
2025-12-25 12:32:44,454 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 12:32:44,454 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 12:32:44,454 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 12:32:44,454 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 12:32:44,454 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 12:32:44,454 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 12:32:44,454 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 12:32:44,454 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 12:32:44,454 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 12:32:44,462 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 12:32:44,470 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 12:32:44,491 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 12:32:44,516 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 12:32:44,516 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 12:32:44,522 [INFO] base.py:214 - Scheduler started
2025-12-25 12:32:44,522 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 12:32:44,540 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 12:32:44,540 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 12:32:44,540 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 12:33:01,263 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:33:01,268 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:33:01,268 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1133601a0>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1132daad0>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:33:01,269 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:33:01,270 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:33:01 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:33:01,270 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:33:01 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:33:01,271 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:33:01,271 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:33:01,272 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:33:01,272 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:33:01,272 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:33:01,272 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:33:01,272 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:33:06,177 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7f2da94f-7bf7-41e6-b9ab-8e50be394dfa', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:33:06,177 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:33:06,178 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:33:06,179 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11388efd0>
2025-12-25 12:33:06,179 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:33:06,179 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:33:06,180 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:33:06,180 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:33:06,180 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:33:06,182 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:33:06 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:33:06,182 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:33:06,182 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:33:06 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:33:06,182 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:33:06,182 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:33:07,037 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:33:07,037 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:33:07,039 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:33:26,735 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-e958e8b9-55db-43fa-9b55-ba7f9fc37d3b', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': "Hello! üòä How can I assist you today? Whether you have questions, need help with something specific, or just want to chat, feel free to let me know‚ÄîI'm here for you!", 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:33:26,735 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:33:26,735 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:33:26,737 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113921e00>
2025-12-25 12:33:26,737 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:33:26,738 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:33:26,738 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:33:26,738 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:33:26,738 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:33:26,741 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:33:26 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:33:26,741 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:33:26,741 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:33:26 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:33:26,741 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:33:26,741 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:33:27,548 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:33:27,549 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:33:27,551 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:33:52,739 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6eee9509-db6d-4cd0-b469-317c6b13bf8a', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': "Hello! üòä How can I assist you today? Whether you have questions, need help with something specific, or just want to chat, feel free to let me know‚ÄîI'm here for you!", 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}, {'content': 'The capital of the **United States** is **Washington, D.C.** (Washington, District of Columbia).\n\nWould you like any more details about it? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the biggest city by population?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:33:52,739 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:33:52,739 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:33:52,741 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1139236f0>
2025-12-25 12:33:52,741 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:33:52,741 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:33:52,741 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:33:52,741 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:33:52,741 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:33:52,745 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:33:52 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:33:52,745 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:33:52,745 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:33:52 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:33:52,745 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:33:52,745 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:33:54,255 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:33:54,255 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:33:54,258 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:34:13,177 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 12:34:20,674 [INFO] main.py:10 - Started
2025-12-25 12:34:20,674 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 12:34:20,674 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 12:34:20,674 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 12:34:20,675 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 12:34:20,675 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 12:34:20,675 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 12:34:20,675 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 12:34:20,675 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 12:34:20,675 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 12:34:20,717 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 12:34:20,726 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 12:34:20,745 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 12:34:20,774 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 12:34:20,774 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 12:34:20,780 [INFO] base.py:214 - Scheduler started
2025-12-25 12:34:20,780 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 12:34:20,840 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 12:34:20,840 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 12:34:20,841 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 12:34:27,243 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:34:27,248 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:34:27,250 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111823a10>
2025-12-25 12:34:27,251 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:34:27,251 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:34:27,251 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:34:27,251 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:34:27,252 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:34:27,252 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1118da210>
2025-12-25 12:34:27,252 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:34:27,252 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:34:27,252 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:34:27,253 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:34:27,253 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:34:27,253 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:34:27 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:34:27,254 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:34:27,254 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:34:27,254 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:34:27,255 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:34:27,255 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:34:27,255 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:34:27,255 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:34:27,255 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:34:27 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:34:27,256 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:34:27,256 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:34:27,256 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:34:27,256 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:34:27,256 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:34:27,257 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:34:27,257 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:34:51,871 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d0ca8c5e-6f07-45a7-996d-205002f4d2a9', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:34:51,871 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:34:51,872 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:34:51,875 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111f7df90>
2025-12-25 12:34:51,876 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:34:51,876 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:34:51,876 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:34:51,877 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:34:51,877 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:34:51,880 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:34:51 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:34:51,880 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:34:51,880 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:34:51 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:34:51,880 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:34:51,881 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:34:52,248 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:34:52,248 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:34:52,248 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:35:56,430 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8cc97bdf-94c9-4b6a-8765-e86e4a5277ae', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi there! How can I help you today? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:35:56,431 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:35:56,432 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:35:56,441 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111fc0c30>
2025-12-25 12:35:56,441 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:35:56,442 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:35:56,442 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:35:56,442 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:35:56,442 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:35:56,446 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:35:56 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:35:56,447 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:35:56,447 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:35:56 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:35:56,447 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:35:56,447 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:35:56,957 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:35:56,958 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:35:56,958 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:37:52,649 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-5af8ea02-cc2c-4ffa-9237-b95e4ba48955', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi there! How can I help you today? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}, {'content': 'The capital of the United States is **Washington, D.C.** (District of Columbia).', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the biggest city by population?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:37:52,652 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:37:52,662 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:37:52,674 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111fc0e90>
2025-12-25 12:37:52,676 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:37:52,680 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:37:52,681 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:37:52,682 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:37:52,682 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:37:52,686 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:37:52 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:37:52,688 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:37:52,689 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:37:52 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:37:52,690 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:37:52,690 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:37:54,342 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:37:54,343 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:37:54,344 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:43:16,345 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 12:43:16,345 [INFO] main.py:10 - Started
2025-12-25 12:43:16,345 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 12:43:16,345 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 12:43:16,345 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 12:43:16,345 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 12:43:16,345 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 12:43:16,345 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 12:43:16,345 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 12:43:16,345 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 12:43:16,345 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 12:43:16,353 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 12:43:16,360 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 12:43:16,378 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 12:43:16,403 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 12:43:16,403 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 12:43:16,408 [INFO] base.py:214 - Scheduler started
2025-12-25 12:43:16,408 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 12:43:16,418 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 12:43:16,418 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 12:43:16,419 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 12:43:29,086 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:43:29,091 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:43:29,092 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10ee501a0>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10edcaad0>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:43:29,093 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:43:29 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:43:29,094 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:43:29,094 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:43:29 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:43:29,095 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:43:29,095 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:43:32,723 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fe34989b-5cc3-46a2-8535-0fc61adcbdc8', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:43:32,724 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:43:32,724 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:43:32,726 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f37f110>
2025-12-25 12:43:32,726 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:43:32,726 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:43:32,726 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:43:32,726 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:43:32,726 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:43:32,728 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:43:32 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:43:32,728 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:43:32,728 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:43:32 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:43:32,728 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:43:32,728 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:43:33,559 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:43:33,559 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:43:33,559 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:45:26,776 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 12:45:28,907 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 12:45:28,907 [INFO] main.py:10 - Started
2025-12-25 12:45:28,907 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 12:45:28,907 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 12:45:28,907 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 12:45:28,907 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 12:45:28,907 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 12:45:28,907 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 12:45:28,907 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 12:45:28,907 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 12:45:28,907 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 12:45:28,914 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 12:45:28,922 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 12:45:28,942 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 12:45:28,968 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 12:45:28,968 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 12:45:28,973 [INFO] base.py:214 - Scheduler started
2025-12-25 12:45:28,973 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 12:45:28,984 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 12:45:28,984 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 12:45:28,984 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 12:45:45,145 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:45:45,150 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:45:45,151 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1136c01a0>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11363ae90>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:45:45,152 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:45:45 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:45:45,153 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:45:45,153 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:45:45 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:45:45,154 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:45:45,154 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:45:48,552 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a30c78cb-8537-43a8-9a5c-8c45702d49f5', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:45:48,552 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:45:48,552 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:45:48,554 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113cab110>
2025-12-25 12:45:48,554 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:45:48,554 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:45:48,554 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:45:48,554 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:45:48,554 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:45:48,557 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:45:48 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:45:48,557 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:45:48,557 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:45:48 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:45:48,557 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:45:48,557 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:45:48,868 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:45:48,868 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:45:48,870 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:45:54,794 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-bb68a68a-7c6d-49a1-8a1a-151603af9552', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! üòä How can I help you today?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:45:54,794 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:45:54,795 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:45:54,796 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113d41cd0>
2025-12-25 12:45:54,796 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:45:54,796 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:45:54,796 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:45:54,796 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:45:54,796 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:45:54,798 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:45:54 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:45:54,798 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:45:54,798 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:45:54 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:45:54,798 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:45:54,798 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:45:55,639 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:45:55,640 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:45:55,641 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:46:11,759 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-316a7668-fadb-4417-abc3-ebdd5e6f0df3', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! üòä How can I help you today?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}, {'content': 'The capital of the United States (USA) is **Washington, D.C.** (Washington, District of Columbia).\n\nWould you like any more details about it? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the biggest city by population?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:46:11,759 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:46:11,760 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:46:11,761 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113d435c0>
2025-12-25 12:46:11,761 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:46:11,761 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:46:11,761 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:46:11,761 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:46:11,761 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:46:11,763 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:46:11 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:46:11,764 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:46:11,764 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:46:11 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:46:11,764 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:46:11,764 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:46:13,808 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:46:13,808 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:46:13,810 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:46:18,745 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 12:46:25,956 [INFO] main.py:10 - Started
2025-12-25 12:46:25,956 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 12:46:25,956 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 12:46:25,956 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 12:46:25,957 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 12:46:25,957 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 12:46:25,957 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 12:46:25,957 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 12:46:25,957 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 12:46:25,957 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 12:46:26,000 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 12:46:26,009 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 12:46:26,028 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 12:46:26,057 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 12:46:26,057 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 12:46:26,063 [INFO] base.py:214 - Scheduler started
2025-12-25 12:46:26,063 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 12:46:26,121 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 12:46:26,121 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 12:46:26,121 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 12:46:32,816 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:46:32,822 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 12:46:32,824 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111947a10>
2025-12-25 12:46:32,824 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:46:32,825 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:46:32,825 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:46:32,825 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:46:32,825 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:46:32,826 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1119fe210>
2025-12-25 12:46:32,826 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 12:46:32,826 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:46:32,826 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 12:46:32,826 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:46:32,827 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 12:46:32,827 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:46:32 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:46:32,828 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:46:32,828 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:46:32,828 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:46:32,829 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:46:32,829 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:46:32,829 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:46:32,829 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:46:32,829 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 09:46:32 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 12:46:32,830 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 12:46:32,830 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 12:46:32,830 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 12:46:32,830 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:46:32,830 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:46:32,831 [DEBUG] _trace.py:87 - close.started
2025-12-25 12:46:32,831 [DEBUG] _trace.py:87 - close.complete
2025-12-25 12:46:47,453 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1d7400f8-3bcf-4a98-b5be-325d35afb8af', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:46:47,454 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:46:47,454 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:46:47,459 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1120a1f90>
2025-12-25 12:46:47,459 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:46:47,460 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:46:47,460 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:46:47,460 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:46:47,461 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:46:47,464 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:46:47 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:46:47,464 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:46:47,464 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:46:47 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:46:47,464 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:46:47,465 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:46:47,824 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:46:47,824 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:46:47,824 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:48:06,668 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2d5af164-555b-4a49-b28e-3c880a06e139', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! How can I assist you today? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:48:06,669 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:48:06,671 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:48:06,677 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11214d480>
2025-12-25 12:48:06,677 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:48:06,681 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:48:06,681 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:48:06,682 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:48:06,682 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:48:06,686 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:48:06 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:48:06,686 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:48:06,687 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:48:06 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:48:06,687 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:48:06,688 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:48:07,626 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:48:07,626 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:48:07,627 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:51:01,259 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f80025d1-e993-41e7-9319-a228a78bd107', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! How can I assist you today? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}, {'content': 'The capital of the United States of America (USA) is **Washington, D.C.**‚Äîofficially known as *District of Columbia* (D.C.).\n\nWould you like to know more about it?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the biggest city by population?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:51:01,260 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:51:01,264 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:51:01,271 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11214c9d0>
2025-12-25 12:51:01,272 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:51:01,273 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:51:01,273 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:51:01,274 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:51:01,275 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:51:01,278 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:51:01 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:51:01,279 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:51:01,279 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:51:01 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:51:01,280 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:51:01,280 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:51:03,550 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:51:03,551 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:51:03,551 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 12:55:02,989 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-97b20e13-a26d-42b9-88f8-dd558bdab6ee', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! How can I assist you today? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}, {'content': 'The capital of the United States of America (USA) is **Washington, D.C.**‚Äîofficially known as *District of Columbia* (D.C.).\n\nWould you like to know more about it?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the biggest city by population?', 'role': 'user'}, {'content': 'As of the latest estimates (2024), the **most populous city in the U.S.** is **New York City** (the "Big Apple") with an estimated population of around **8.5 million** within its city limits.\n\nIf you\'re comparing metropolitan areas, **Los Angeles** and **Chicago** also have very large populations (~3.9M and ~2.7M in the city itself), but NYC remains the largest standalone metropolis.\n\nWould you like data on a specific region or another country?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is top 5?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 12:55:02,992 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 12:55:02,999 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 12:55:03,012 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1121797f0>
2025-12-25 12:55:03,013 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 12:55:03,016 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 12:55:03,017 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 12:55:03,018 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 12:55:03,018 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 12:55:03,022 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 09:55:03 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 12:55:03,024 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 12:55:03,025 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 09:55:03 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 12:55:03,025 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 12:55:03,026 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 12:55:07,983 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 12:55:07,984 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 12:55:07,985 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 13:01:35,835 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-aa883b11-c770-4f67-a9d9-d3952f39b75f', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! How can I assist you today? üòä', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the capital of USA?', 'role': 'user'}, {'content': 'The capital of the United States of America (USA) is **Washington, D.C.**‚Äîofficially known as *District of Columbia* (D.C.).\n\nWould you like to know more about it?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is the biggest city by population?', 'role': 'user'}, {'content': 'As of the latest estimates (2024), the **most populous city in the U.S.** is **New York City** (the "Big Apple") with an estimated population of around **8.5 million** within its city limits.\n\nIf you\'re comparing metropolitan areas, **Los Angeles** and **Chicago** also have very large populations (~3.9M and ~2.7M in the city itself), but NYC remains the largest standalone metropolis.\n\nWould you like data on a specific region or another country?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'What is top 5?', 'role': 'user'}, {'content': 'Here‚Äôs the **top 5 most populous U.S. cities** (based on 2024 estimates, using the latest Census Bureau data and metro area population):\n\n1. **New York City, NY** ‚Äì ~8.5 million\n   *(Metro population: ~20+ million)*\n\n2. **Los Angeles, CA** ‚Äì ~3.9 million\n   *(Metro population: ~13+ million)*\n\n3. **Chicago, IL** ‚Äì ~2.7 million\n   *(Metro population: ~9.5 million)*\n\n4. **Houston, TX** ‚Äì ~2.3 million\n   *(Metro population: ~7.1 million)*\n\n5. **Phoenix, AZ** ‚Äì ~1.6 million (growing rapidly!)\n\n### Notes:\n- *City populations* exclude metro areas.\n- For the most accurate rankings, check recent Census data or sources like [U.S. Census Bureau](https://www.census.gov).\n- If you‚Äôre focusing on **total population in metropolitan statistical areas**, Houston, Dallas-Fort Worth, and Philadelphia often rank higher.\n\nWould you like a deeper breakdown?', 'role': 'assistant'}, {'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Deeper breakdown for number 4', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 13:01:35,837 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 13:01:35,843 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 13:01:35,859 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1120617b0>
2025-12-25 13:01:35,860 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 13:01:35,862 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 13:01:35,862 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 13:01:35,863 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 13:01:35,863 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 13:01:35,868 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 10:01:35 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 13:01:35,870 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 13:01:35,870 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 10:01:35 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 13:01:35,871 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 13:01:35,872 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 13:01:49,039 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 13:01:49,040 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 13:01:49,041 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 13:59:40,147 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 13:59:40,147 [INFO] main.py:10 - Started
2025-12-25 13:59:40,147 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 13:59:40,147 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 13:59:40,147 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 13:59:40,147 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 13:59:40,147 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 13:59:40,147 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 13:59:40,147 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 13:59:40,147 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 13:59:40,147 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 13:59:40,155 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 13:59:40,175 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 13:59:40,197 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 13:59:40,226 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 13:59:40,226 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 13:59:40,231 [INFO] base.py:214 - Scheduler started
2025-12-25 13:59:40,231 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 13:59:40,256 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 13:59:40,257 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 13:59:40,257 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 14:00:02,975 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:00:02,981 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:00:02,981 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x112f1d400>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x112e87390>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:00:02,982 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:00:02,983 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:00:02,983 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:00:02 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:00:02,983 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:00:02,983 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:00:02,983 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:00:02,983 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:00:02,983 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:00:02 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:00:02,984 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:00:02,984 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:00:27,831 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-cf5dc593-049b-49d3-b3c9-b28f6a930dc3', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:00:27,831 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:00:27,832 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:00:27,833 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11354c190>
2025-12-25 14:00:27,833 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:00:27,834 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:00:27,834 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:00:27,834 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:00:27,834 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:00:27,836 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:00:27 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:00:27,837 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:00:27,837 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:00:27 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:00:27,837 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:00:27,837 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:00:28,514 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:00:28,514 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:00:28,516 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:00:35,738 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-6a131b66-9011-4b5a-9a84-299c5c2ca30b', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you know my cat's name?", 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:00:35,738 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:00:35,739 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:00:35,740 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1134ae650>
2025-12-25 14:00:35,740 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:00:35,741 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:00:35,741 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:00:35,741 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:00:35,741 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:00:35,744 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:00:35 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:00:35,744 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:00:35,744 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:00:35 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:00:35,744 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:00:35,744 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:00:36,516 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:00:36,517 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:00:36,518 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:00:36,536 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-8bed4f68-70de-4b64-a86f-82782a7d9720', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you know my cat's name?", 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '869755920', 'function': {'name': 'search_memory', 'arguments': '{"query": "cat\'s name"}'}}]}, {'content': 'No relevant memories found.', 'role': 'tool', 'tool_call_id': '869755920'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:00:36,537 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:00:36,537 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:00:36,540 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1134afe10>
2025-12-25 14:00:36,540 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:00:36,540 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:00:36,540 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:00:36,540 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:00:36,540 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:00:36,541 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:00:36 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:00:36,541 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:00:36,541 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:00:36 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:00:36,541 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:00:36,541 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:00:37,303 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:00:37,303 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:00:37,305 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:01:02,713 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-53408e4a-0674-452a-947f-0803a4ef4fe4', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you know my cat's name?", 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '869755920', 'function': {'name': 'search_memory', 'arguments': '{"query": "cat\'s name"}'}}]}, {'content': 'No relevant memories found.', 'role': 'tool', 'tool_call_id': '869755920'}, {'content': "I don't have any prior information about your cat's name from our conversation history. Could you tell me what it is so I can remember it for future interactions? üòä", 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'It is Stinpy', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:01:02,713 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:01:02,713 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:01:02,714 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113580b90>
2025-12-25 14:01:02,714 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:01:02,714 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:02,715 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:01:02,715 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:02,715 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:01:02,717 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:01:02 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:01:02,717 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:01:02,717 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:01:02 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:01:02,717 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:01:02,717 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:01:03,793 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:03,794 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:03,795 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:01:03,815 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3ebdea57-be40-4c9d-af15-114468b725ba', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you know my cat's name?", 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '869755920', 'function': {'name': 'search_memory', 'arguments': '{"query": "cat\'s name"}'}}]}, {'content': 'No relevant memories found.', 'role': 'tool', 'tool_call_id': '869755920'}, {'content': "I don't have any prior information about your cat's name from our conversation history. Could you tell me what it is so I can remember it for future interactions? üòä", 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'It is Stinpy', 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '455451226', 'function': {'name': 'save_memory', 'arguments': '{"key": "cat_name", "value": "Stinpy"}'}}]}, {'content': 'Saved memory: cat_name = Stinpy', 'role': 'tool', 'tool_call_id': '455451226'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:01:03,815 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:01:03,816 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:01:03,817 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11356c380>
2025-12-25 14:01:03,817 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:01:03,817 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:03,817 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:01:03,818 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:03,818 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:01:03,818 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:01:03 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:01:03,819 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:01:03,819 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:01:03 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:01:03,819 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:01:03,819 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:01:04,454 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:04,454 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:04,456 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:01:12,972 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:01:12,979 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11356e7a0>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11357ca50>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:12,980 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:01:12 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:01:12,981 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:01:12,981 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:01:12 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:01:12,982 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:01:12,982 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:01:19,292 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2ecc64eb-9dbe-45e4-a84e-14581f455a3a', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hey there', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:01:19,292 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:01:19,293 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:01:19,294 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11357d050>
2025-12-25 14:01:19,294 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:01:19,294 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:19,294 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:01:19,294 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:19,294 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:01:19,297 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:01:19 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:01:19,297 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:01:19,297 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:01:19 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:01:19,297 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:01:19,297 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:01:19,819 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:19,819 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:19,821 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:01:28,828 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-60dde68e-65ab-4a32-9500-05667611877f', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hey there', 'role': 'user'}, {'content': 'Hello! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you know my cat's name?", 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:01:28,828 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:01:28,828 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:01:28,830 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11353e300>
2025-12-25 14:01:28,830 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:01:28,830 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:28,830 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:01:28,830 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:28,830 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:01:28,833 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:01:28 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:01:28,833 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:01:28,833 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:01:28 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:01:28,833 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:01:28,833 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:01:29,602 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:29,602 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:29,604 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:01:29,623 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-85cf2c1f-38ad-4b36-9961-2ada32013733', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hey there', 'role': 'user'}, {'content': 'Hello! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you know my cat's name?", 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '167894707', 'function': {'name': 'search_memory', 'arguments': '{"query": "cat\'s name"}'}}]}, {'content': 'cat_name: Stinpy', 'role': 'tool', 'tool_call_id': '167894707'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:01:29,623 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:01:29,624 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:01:29,625 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11353fc50>
2025-12-25 14:01:29,626 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:01:29,626 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:01:29,626 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:01:29,626 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:01:29,626 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:01:29,627 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:01:29 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:01:29,627 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:01:29,627 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:01:29 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:01:29,627 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:01:29,627 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:01:30,271 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:01:30,272 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:01:30,274 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:03:01,644 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 14:03:12,525 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 14:03:12,525 [INFO] main.py:10 - Started
2025-12-25 14:03:12,525 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 14:03:12,525 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 14:03:12,525 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 14:03:12,525 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 14:03:12,525 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 14:03:12,525 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 14:03:12,525 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 14:03:12,525 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 14:03:12,525 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 14:03:12,533 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 14:03:12,541 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 14:03:12,561 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 14:03:12,586 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 14:03:12,587 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 14:03:12,592 [INFO] base.py:214 - Scheduler started
2025-12-25 14:03:12,592 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 14:03:12,607 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 14:03:12,607 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 14:03:12,607 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 14:03:22,386 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:03:22,392 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:03:22,393 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1118bd400>
2025-12-25 14:03:22,393 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111822d50>
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:03:22,394 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:03:22,395 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:03:22 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:03:22,395 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:03:22,395 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:03:22,395 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:03:22,395 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:03:22,395 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:03:22,395 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:03:22 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:03:22,396 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:03:22,396 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:03:25,083 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1f42cffe-9c88-421a-834a-e4fc3146dba8', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:03:25,083 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:03:25,083 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:03:25,085 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111de7890>
2025-12-25 14:03:25,085 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:03:25,086 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:03:25,086 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:03:25,086 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:03:25,086 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:03:25,088 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:03:25 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:03:25,088 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:03:25,088 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:03:25 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:03:25,088 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:03:25,088 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:03:25,596 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:03:25,597 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:03:25,598 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:03:35,038 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-205cbfa9-5843-4b0b-adfa-2496944e8b18', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi there! How can I assist you today?', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you remember my cat's name?", 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:03:35,038 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:03:35,039 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:03:35,040 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111e4a520>
2025-12-25 14:03:35,040 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:03:35,040 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:03:35,040 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:03:35,040 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:03:35,040 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:03:35,043 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:03:35 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:03:35,043 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:03:35,043 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:03:35 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:03:35,043 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:03:35,043 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:03:35,833 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:03:35,833 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:03:35,835 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:03:35,855 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-d600eb88-3921-4a94-8ae3-5dc4722b5594', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi there! How can I assist you today?', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': "Do you remember my cat's name?", 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '823368628', 'function': {'name': 'search_memory', 'arguments': '{"query": "cat\'s name"}'}}]}, {'content': 'cat_name: Stinpy', 'role': 'tool', 'tool_call_id': '823368628'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:03:35,855 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:03:35,855 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:03:35,858 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111e4be10>
2025-12-25 14:03:35,858 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:03:35,858 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:03:35,858 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:03:35,858 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:03:35,858 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:03:35,859 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:03:35 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:03:35,859 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:03:35,859 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:03:35 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:03:35,859 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:03:35,859 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:03:36,442 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:03:36,443 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:03:36,444 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:45:40,245 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:45:40,250 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:45:40,250 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111f20cb0>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111f286b0>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:45:40,251 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:45:40,252 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:45:40 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:45:40,252 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:45:40,252 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:45:40,252 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:45:40,252 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:45:40,252 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:45:40 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:45:40,253 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:45:40,253 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:46:02,723 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:46:02,729 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:46:02,730 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111f2a690>
2025-12-25 14:46:02,730 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:46:02,730 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:46:02,730 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:46:02,730 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:46:02,730 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:46:02,731 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111edcf50>
2025-12-25 14:46:02,731 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:46:02,731 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:46:02,731 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:46:02,731 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:46:02,731 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:46:02 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:46:02,732 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:46:02,732 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:46:02 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:46:02,733 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:46:02,733 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:47:55,018 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 14:47:57,246 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 14:47:57,246 [INFO] main.py:10 - Started
2025-12-25 14:47:57,246 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 14:47:57,246 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 14:47:57,246 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 14:47:57,246 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 14:47:57,247 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 14:47:57,247 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 14:47:57,247 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 14:47:57,247 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 14:47:57,247 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 14:47:57,255 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 14:47:57,264 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 14:47:57,284 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 14:47:57,310 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 14:47:57,310 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 14:47:57,315 [INFO] base.py:214 - Scheduler started
2025-12-25 14:47:57,315 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 14:47:57,333 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 14:47:57,333 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 14:47:57,333 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 14:48:11,493 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:48:11,493 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:48:11,494 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x113a79400>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1139d2fd0>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:48:11,495 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:48:11 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:48:11,497 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:48:11,497 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:48:11 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:48:11,498 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:48:11,498 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:49:04,280 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 14:49:06,369 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 14:49:06,369 [INFO] main.py:10 - Started
2025-12-25 14:49:06,369 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 14:49:06,369 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 14:49:06,369 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 14:49:06,369 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 14:49:06,369 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 14:49:06,369 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 14:49:06,369 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 14:49:06,369 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 14:49:06,369 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 14:49:06,377 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 14:49:06,385 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 14:49:06,404 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 14:49:06,430 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 14:49:06,430 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 14:49:06,435 [INFO] base.py:214 - Scheduler started
2025-12-25 14:49:06,435 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 14:49:06,451 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 14:49:06,451 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 14:49:06,451 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 14:49:10,745 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:49:10,746 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10ee6d160>
2025-12-25 14:49:10,746 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:49:10,746 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:49:10,746 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:49:10,746 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:49:10,746 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:49:10,748 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:49:10 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:49:10,748 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:49:10,748 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:49:10,748 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:49:10,748 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:49:10,748 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:49:10,749 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:49:10,749 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:49:10,756 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:49:10,757 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10edd2c10>
2025-12-25 14:49:10,757 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:49:10,757 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:49:10,758 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:49:10,758 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:49:10,758 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:49:10,759 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:49:10 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:49:10,759 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:49:10,759 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:49:10,759 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:49:10,759 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:49:10,759 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:49:10,760 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:49:10,760 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:51:09,562 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 14:51:11,291 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 14:51:11,291 [INFO] main.py:10 - Started
2025-12-25 14:51:11,291 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 14:51:11,291 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 14:51:11,291 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 14:51:11,291 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 14:51:11,291 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 14:51:11,291 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 14:51:11,291 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 14:51:11,291 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 14:51:11,291 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 14:51:11,298 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 14:51:11,306 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 14:51:11,324 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 14:51:11,349 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 14:51:11,349 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 14:51:11,354 [INFO] base.py:214 - Scheduler started
2025-12-25 14:51:11,354 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 14:51:11,365 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 14:51:11,365 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 14:51:11,365 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 14:51:22,790 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:51:22,794 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f4152b0>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f37b110>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:22,795 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:51:22 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:51:22,796 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:51:22,796 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:51:22 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:51:22,796 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:51:22,797 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:51:22,797 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:51:22,797 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:22,797 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:22,797 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:51:22,797 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:51:32,361 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:51:32,363 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f9bee90>
2025-12-25 14:51:32,363 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:51:32,363 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:32,363 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:51:32,363 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:32,363 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:51:32 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:51:32,364 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:51:32,364 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:51:32,372 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:51:32,373 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10fa62190>
2025-12-25 14:51:32,373 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:51:32,373 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:32,373 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:51:32,373 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:32,373 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:51:32 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:51:32,374 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:51:32,374 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:51:36,705 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2dd63cc1-fcd7-4c1a-9bc3-fbee170eba37', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:51:36,706 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:51:36,706 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:51:36,708 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10fa60e90>
2025-12-25 14:51:36,708 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:51:36,708 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:36,708 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:51:36,708 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:36,708 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:51:36,712 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:51:36 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:51:36,712 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:51:36,712 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:51:36 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:51:36,712 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:51:36,712 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:51:37,435 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:37,436 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:37,438 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:51:43,659 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-a74eebc9-984d-4b44-a023-393be733a710', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi there! How can I assist you today? üòä', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'How are you today?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 14:51:43,659 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 14:51:43,659 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 14:51:43,660 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10fb284d0>
2025-12-25 14:51:43,660 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 14:51:43,661 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:43,661 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 14:51:43,661 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:43,661 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 14:51:43,664 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 11:51:43 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 14:51:43,664 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 14:51:43,664 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 11:51:43 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 14:51:43,664 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 14:51:43,664 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 14:51:44,876 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:44,877 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:44,877 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 14:51:51,419 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:51:51,437 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10faf17b0>
2025-12-25 14:51:51,437 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:51:51,437 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:51,437 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:51:51,437 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:51,437 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:51:51 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:51:51,438 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:51:51,438 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:51:51,448 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:51:51,449 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10faf2690>
2025-12-25 14:51:51,449 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:51:51,449 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:51:51,449 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:51:51,449 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:51:51,449 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:51:51 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:51:51,451 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:51:51,451 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:52:09,180 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:52:09,180 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 14:52:09,181 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10fb1cb50>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10fb1e650>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 14:52:09,182 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 14:52:09,183 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:52:09 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:52:09,183 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:52:09,183 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:52:09,183 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:52:09,183 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:52:09,183 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:52:09,183 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-FhX1pO7RaUTakkZ6hbZEQ0UO5+M"'), (b'Date', b'Thu, 25 Dec 2025 11:52:09 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 14:52:09,184 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - close.started
2025-12-25 14:52:09,184 [DEBUG] _trace.py:87 - close.complete
2025-12-25 14:52:17,466 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-202' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 14:52:20,906 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-29' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:20,906 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-87' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:20,906 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-181' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:20,906 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-200' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:20,906 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-204' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:20,906 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-206' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:34,070 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-219' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 14:52:56,463 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-211' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,463 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-213' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,463 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-215' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,463 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-217' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,463 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-221' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,463 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-223' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,464 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-225' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,464 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-228' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,464 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-230' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,464 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-233' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 14:52:56,464 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-235' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:07:24,324 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 15:07:36,303 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 15:07:36,303 [INFO] main.py:10 - Started
2025-12-25 15:07:36,303 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 15:07:36,303 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 15:07:36,303 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 15:07:36,303 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 15:07:36,303 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 15:07:36,303 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 15:07:36,303 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 15:07:36,303 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 15:07:36,303 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 15:07:36,312 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 15:07:36,321 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 15:07:36,341 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 15:07:36,366 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 15:07:36,366 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 15:07:36,371 [INFO] base.py:214 - Scheduler started
2025-12-25 15:07:36,371 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 15:07:36,388 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 15:07:36,388 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 15:07:36,389 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 15:07:47,033 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:07:47,033 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:07:47,034 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11145d400>
2025-12-25 15:07:47,034 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:07:47,034 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:07:47,034 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1113b7750>
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:07:47,035 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:07:47 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:07:47,037 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:07:47,037 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:07:47 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:07:47,038 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:07:47,038 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:08:43,035 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:08:43,035 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:08:43,036 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11146ca50>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111417230>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:08:43,037 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:08:43 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:08:43,038 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:08:43,038 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:08:43 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:08:43,039 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:08:43,039 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:09:07,530 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 15:11:00,814 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 15:11:00,814 [INFO] main.py:10 - Started
2025-12-25 15:11:00,814 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 15:11:00,814 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 15:11:00,814 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 15:11:00,814 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 15:11:00,814 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 15:11:00,814 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 15:11:00,814 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 15:11:00,814 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 15:11:00,814 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 15:11:00,822 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 15:11:00,829 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 15:11:00,848 [INFO] postgresql_manager.py:29 - Trying to activate connection id: 1
2025-12-25 15:11:00,873 [ERROR] postgresql_connection.py:56 - ‚ùå Connection failed for id=1: Multiple exceptions: [Errno 61] Connect call failed ('::1', 5432, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 5432)
2025-12-25 15:11:00,873 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 15:11:00,878 [INFO] base.py:214 - Scheduler started
2025-12-25 15:11:00,878 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 15:11:00,888 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 15:11:00,888 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 15:11:00,888 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 15:11:10,504 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:11:10,508 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:11:10,509 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11163d2b0>
2025-12-25 15:11:10,509 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:11:10,509 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:11:10,509 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:11:10,509 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:11:10,509 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:11:10,510 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11159f610>
2025-12-25 15:11:10,510 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:11:10,510 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:11:10,510 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:11:10,510 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:11:10,510 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:11:10,511 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:11:10 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:11:10,512 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:11:10 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:11:10,512 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:11:10,512 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:11:10,513 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:11:10,513 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:11:10,513 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:11:10,513 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:11:10,513 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:11:52,378 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:11:52,379 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d65590>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d70b00>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:11:52,380 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:11:52,381 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:11:52 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:11:52,382 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:11:52,382 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:11:52 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:11:52,383 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:11:52,383 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:11:52,383 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:11:52,383 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:11:52,383 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:11:52,383 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:11:52,383 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:12:01,142 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4bb7d1d7-2a28-47d4-adc5-642b0b168eb3', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:12:01,143 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:12:01,143 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:12:01,145 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d70180>
2025-12-25 15:12:01,145 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:12:01,145 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:01,145 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:12:01,145 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:01,145 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:12:01,148 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:12:01 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:12:01,148 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:12:01,148 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:12:01 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:12:01,148 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:12:01,149 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:12:01,616 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:01,617 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:01,618 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:12:01,642 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-66b8c735-ce34-48fa-b84c-bbce90f9c955', 'json_data': {'messages': [{'content': 'Summarize the following message into a concise 3-5 word title. Do not use quotes. Message: Hello', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 15:12:01,642 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:12:01,643 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:12:01,644 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d75130>
2025-12-25 15:12:01,644 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:12:01,645 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:01,645 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:12:01,645 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:01,645 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:12:01,647 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:12:01 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:12:01,647 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:12:01,647 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:12:01 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:12:01,647 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:12:01,647 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:12:02,470 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:02,470 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:02,473 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:12:21,747 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b279be45-9f4e-4fc1-8b4f-fa40fa9e8afb', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! How can I assist you today?', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'I would like to know GDP of top 10 countries', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:12:21,748 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:12:21,748 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:12:21,749 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111c7d8c0>
2025-12-25 15:12:21,749 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:12:21,749 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:21,749 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:12:21,749 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:21,749 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:12:21,753 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:12:21 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:12:21,753 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:12:21,753 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:12:21 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:12:21,753 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:12:21,753 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:12:22,755 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:22,756 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:22,757 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:12:22,776 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b25773ef-331b-4569-89aa-bc0d5f3769b6', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hi! How can I assist you today?', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'I would like to know GDP of top 10 countries', 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '355738324', 'function': {'name': 'search_memory', 'arguments': '{"query": "GDP of top 10 countries"}'}}]}, {'content': 'cat_name: Stinpy', 'role': 'tool', 'tool_call_id': '355738324'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:12:22,777 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:12:22,777 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:12:22,780 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111dd87c0>
2025-12-25 15:12:22,780 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:12:22,780 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:22,780 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:12:22,780 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:22,780 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:12:22,781 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:12:22 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:12:22,781 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:12:22,781 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:12:22 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:12:22,781 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:12:22,781 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:12:27,935 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:27,935 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:27,939 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:12:39,307 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:12:39,308 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d8d650>
2025-12-25 15:12:39,308 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:12:39,308 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:39,308 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:12:39,308 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:39,308 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:12:39,310 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:12:39 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:12:39,310 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:12:39,310 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:12:39,310 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:12:39,311 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:39,311 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:39,311 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:12:39,311 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:12:39,318 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:12:39,319 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d8ea50>
2025-12-25 15:12:39,319 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:12:39,319 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:39,319 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:12:39,319 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:39,319 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:12:39 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:12:39,321 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:12:39,321 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:12:41,103 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-20' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:12:41,103 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-22' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:12:41,103 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-25' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:12:41,103 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-37' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:12:41,103 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-108' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:12:51,802 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:12:51,803 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x11150b020>
2025-12-25 15:12:51,803 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:12:51,803 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:51,803 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:12:51,803 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:51,803 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:12:51,804 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:12:51 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:12:51,805 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:12:51,805 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:12:51,805 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:12:51,805 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:51,805 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:51,805 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:12:51,805 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:12:51,813 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:12:51,814 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d6e7b0>
2025-12-25 15:12:51,814 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:12:51,814 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:12:51,814 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:12:51,815 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:12:51,815 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:12:51 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:12:51,816 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:12:51,816 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:13:12,170 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-546d4183-562f-4937-81df-a23377597635', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'What is the best beaches to visit in Turkey?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:13:12,170 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:13:12,170 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:13:12,172 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111ca3af0>
2025-12-25 15:13:12,172 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:13:12,172 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:13:12,172 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:13:12,172 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:13:12,172 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:13:12,176 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:13:12 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:13:12,176 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:13:12,176 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:13:12 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:13:12,176 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:13:12,176 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:13:12,925 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:13:12,926 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:13:12,928 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:13:12,947 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-1190922a-819b-4e1d-83e3-51fa012bf74f', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'What is the best beaches to visit in Turkey?', 'role': 'user'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'type': 'function', 'id': '186504871', 'function': {'name': 'search_memory', 'arguments': '{"query": "best beaches in Turkey"}'}}]}, {'content': 'cat_name: Stinpy', 'role': 'tool', 'tool_call_id': '186504871'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:13:12,948 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:13:12,948 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:13:12,950 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d1dfd0>
2025-12-25 15:13:12,950 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:13:12,950 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:13:12,950 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:13:12,950 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:13:12,950 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:13:12,951 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:13:12 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:13:12,951 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:13:12,951 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:13:12 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:13:12,951 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:13:12,951 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:13:37,757 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:13:37,757 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:13:37,766 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:13:37,787 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fa1abdf3-8c43-489e-a4f7-0861108f8700', 'json_data': {'messages': [{'content': 'Summarize the following message into a concise 3-5 word title. Do not use quotes. Message: What is the best beaches to visit in Turkey?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 15:13:37,787 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:13:37,787 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:13:37,789 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111d3e5b0>
2025-12-25 15:13:37,789 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:13:37,789 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:13:37,789 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:13:37,789 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:13:37,789 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:13:37,791 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:13:37 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:13:37,791 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:13:37,791 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:13:37 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:13:37,791 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:13:37,791 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:13:38,730 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:13:38,730 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:13:38,733 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:13:48,511 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:13:48,543 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:13:48,544 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111c0fc50>
2025-12-25 15:13:48,544 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:13:48,544 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:13:48,544 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:13:48,544 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:13:48,544 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111c0f710>
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:13:48 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:13:48,545 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:13:48,545 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:13:48,546 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:13:48,546 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:13:48,547 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:13:48 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:13:48,547 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:13:48,548 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:13:48,548 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:13:48,548 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:13:48,548 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:13:48,548 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:13:48,548 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:13:57,349 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-1910' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 15:20:00,445 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-504' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:00,445 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-1908' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:00,445 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-1912' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:17,328 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-573ef8f5-95dc-45e2-9282-8934fc889314', 'json_data': {'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'}, {'content': 'Hello', 'role': 'user'}, {'content': 'Hello! How can I help you today?', 'role': 'assistant'}, {'content': 'Whatz up man?', 'role': 'user'}, {'content': 'Haha, got it‚Äî**what‚Äôs good?** ‚òÄÔ∏èüòé\n\nHow‚Äôs *your* day treating you? Need help with anything‚Äîcoding, advice, random thoughts, or just vibing? Drop it and let‚Äôs chat! üëä', 'role': 'assistant'}, {'content': 'Yeah, coding bitch', 'role': 'user'}, {'content': 'Haha, **"bitch"** for speed! üöÄ Coding mode: **ON**\n\nGot a project in mind?\n- Need help debugging something?\n- Looking for code snippets or algorithms?\n- Just here to geek out about memes/coding humor? (I‚Äôll respect either! üòÇ)\n\nAsk away‚Äîwhat‚Äôs the *real* question? üíªüî•', 'role': 'assistant'}, {'content': 'Ok time to be serious. Thanks for handling my previous messages kindly :)\nI would like to create a C algorithm for quick sort. How can I start?', 'role': 'user'}, {'content': 'No problem! QuickSort is a classic **divide-and-conquer** algorithm that sorts arrays by partitioning them around a pivot element.\n\n### **Steps to Implement QuickSort in C**\nHere‚Äôs how you can start:\n\n#### **1. Understand the Logic**\n- Choose a pivot (e.g., last element).\n- Partition the array into two subarrays:\n  - Elements ‚â§ pivot (left)\n  - Elements > pivot (right)\n- Recursively sort the left and right partitions.\n\n#### **2. Basic QuickSort Implementation**\n```c\n#include <stdio.h>\n\n// Function to partition the array\nint partition(int arr[], int low, int high) {\n    int pivot = arr[high]; // Choose last element as pivot\n    int i = (low - 1);     // Index of smaller element\n\n    for (int j = low; j <= high - 1; j++) {\n        if (arr[j] <= pivot) { // If current element is ‚â§ pivot\n            i++;             // Move index forward\n            // Swap elements at i and j\n            int temp = arr[i];\n            arr[i] = arr[j];\n            arr[j] = temp;\n        }\n    }\n    // Place pivot in correct position\n    int temp2 = arr[i + 1];\n    arr[i + 1] = arr[high];\n    arr[high] = temp2;\n\n    return (i + 1); // Return the partition index\n}\n\n// QuickSort function\nvoid quickSort(int arr[], int low, int high) {\n    if (low < high) {       // Base case: array has more than one element\n        int pi = partition(arr, low, high); // Partitioning index\n\n        // Recursively sort elements before and after partition\n        quickSort(arr, low, pi - 1);\n        quickSort(arr, pi + 1, high);\n    }\n}\n\n// Helper function to print an array\nvoid printArray(int arr[], int size) {\n    for (int i = 0; i < size; i++)\n        printf("%d ", arr[i]);\n    printf("\\n");\n}\n\n// Main function (driver code)\nint main() {\n    int arr[] = {10, 7, 8, 9, 1, 5};\n    int n = sizeof(arr) / sizeof(arr[0]);\n\n    printf("Original array: ");\n    printArray(arr, n);\n\n    quickSort(arr, 0, n - 1); // Sort the array\n\n    printf("Sorted array:   ");\n    printArray(arr, n);\n    return 0;\n}\n```\n\n#### **3. Key Notes**\n‚úÖ **Partitioning**: Critical step‚Äîchoose pivot wisely (first/last/random).\n‚úÖ **Recursion**: QuickSort is recursive‚Äîbase case must exist (`low < high`).\n‚úÖ **Time Complexity**:\n   - **Best/Average Case**: O(n log n)\n   - **Worst Case** (if pivot is smallest/largest): O(n¬≤) ‚Üí Use random pivot for robustness.\n\n#### **4. Try It Yourself**\n- Compile & run the code.\n- Modify `arr[]` to test different inputs.\n- Experiment with edge cases (empty array, single-element array).\n\nWant to dive deeper? Let me know:\n1. What part confuses you?\n2. Should we optimize further? (e.g., tail recursion, randomized pivot)\n3. Any other questions? üòä\n\n*(Also, if you‚Äôre learning, consider comparing it with MergeSort‚Äîboth are great but behave differently!)*', 'role': 'assistant'}, {'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Yes I want to see an example for MergeSort in C', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:20:17,329 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:20:17,329 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:20:17,330 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111cd0e10>
2025-12-25 15:20:17,330 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:20:17,330 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:20:17,330 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:20:17,330 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:20:17,330 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:20:17,334 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:20:17 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:20:17,334 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:20:17,334 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:20:17 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:20:17,334 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:20:17,334 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:20:37,079 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:20:37,079 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:20:37,087 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:20:37,110 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-12cfc8b6-179a-43bf-a29c-011b936df292', 'json_data': {'messages': [{'content': 'Summarize the following message into a concise 3-5 word title. Do not use quotes. Message: Yes I want to see an example for MergeSort in C', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 15:20:37,110 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:20:37,110 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:20:37,112 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111cd1f40>
2025-12-25 15:20:37,112 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:20:37,112 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:20:37,112 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:20:37,112 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:20:37,112 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:20:37,114 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:20:37 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:20:37,114 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:20:37,114 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:20:37 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:20:37,114 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:20:37,114 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:20:37,997 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:20:37,997 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:20:37,999 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:20:56,246 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2888' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 15:20:58,830 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-1919' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:58,830 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2886' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:58,830 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2890' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:58,830 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2892' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:20:58,830 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2894' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:36:13,004 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2902' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 15:36:16,335 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:36:16,339 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:36:16,356 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111e10410>
2025-12-25 15:36:16,356 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111e116d0>
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:36:16,357 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:36:16 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:36:16,358 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:36:16,358 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:36:16 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:36:16,359 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:36:16,359 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:36:29,600 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2939' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 15:36:29,618 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:36:29,623 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=2.0 socket_options=None
2025-12-25 15:36:29,635 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111dbad50>
2025-12-25 15:36:29,635 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:36:29,641 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:36:29,641 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:36:29,641 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:36:29,641 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:36:29,641 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111dc6150>
2025-12-25 15:36:29,641 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'GET']>
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'GET']>
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'GET']>
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:36:29 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:36:29,642 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:36:29,642 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:36:29,643 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:36:29,643 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'395'), (b'ETag', b'W/"18b-yY8sfmr72AmFCueQoNjrk+MFEdA"'), (b'Date', b'Thu, 25 Dec 2025 12:36:29 GMT'), (b'Connection', b'keep-alive'), (b'Keep-Alive', b'timeout=5')])
2025-12-25 15:36:29,643 [INFO] _client.py:1740 - HTTP Request: GET http://localhost:1234/v1/models "HTTP/1.1 200 OK"
2025-12-25 15:36:29,643 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'GET']>
2025-12-25 15:36:29,643 [DEBUG] _trace.py:87 - receive_response_body.complete
2025-12-25 15:36:29,643 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:36:29,644 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:36:29,644 [DEBUG] _trace.py:87 - close.started
2025-12-25 15:36:29,644 [DEBUG] _trace.py:87 - close.complete
2025-12-25 15:36:33,565 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2960' coro=<_run() done, defined at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:326> wait_for=<Future cancelled>>
2025-12-25 15:43:37,980 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2898' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,980 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2900' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,980 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2935' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,980 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2936' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,980 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2956' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,981 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2957' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,981 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2963' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,981 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2968' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:43:37,981 [ERROR] base_events.py:1879 - Task was destroyed but it is pending!
task: <Task cancelling name='Task-2973' coro=<_run() running at /Users/serdarmutlu/Projects/mcp/python/DBMCPServer/.venv/lib/python3.13/site-packages/langgraph/store/base/batch.py:330> wait_for=<Future cancelled>>
2025-12-25 15:44:08,338 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-97e66c78-237f-4ad9-b07f-80bfb8bbf326', 'json_data': {'messages': [{'content': "You are a helpful assistant.\n\nSYSTEM NOTE: You have access to long-term memory tools ('save_memory', 'search_memory'). Use 'save_memory' to persist important user facts/preferences. Use 'search_memory' to recall them when needed.", 'role': 'system'}, {'content': 'Yukarƒ±daki tuvaleti arka balkona kaldƒ±ralƒ±m mƒ±?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7, 'tools': [{'type': 'function', 'function': {'name': 'save_memory', 'description': 'Save a piece of information to long-term memory. Use concise keys.', 'parameters': {'properties': {'key': {'type': 'string'}, 'value': {'type': 'string'}}, 'required': ['key', 'value'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'search_memory', 'description': 'Search for information in long-term memory.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}}
2025-12-25 15:44:08,338 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:44:08,339 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:44:08,340 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111e45dd0>
2025-12-25 15:44:08,340 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:44:08,340 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:44:08,340 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:44:08,340 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:44:08,340 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:44:08,344 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:44:08 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:44:08,344 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:44:08,344 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:44:08 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:44:08,344 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:44:08,344 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:44:16,203 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:44:16,203 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:44:16,207 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 15:44:16,231 [DEBUG] _base_client.py:482 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-2e3c4a1f-659c-4a2d-ab55-69a2fe3d9fd7', 'json_data': {'messages': [{'content': 'Summarize the following message into a concise 3-5 word title. Do not use quotes. Message: Yukarƒ±daki tuvaleti arka balkona kaldƒ±ralƒ±m mƒ±?', 'role': 'user'}], 'model': 'mistralai/ministral-3-3b', 'stream': True, 'temperature': 0.7}}
2025-12-25 15:44:16,231 [DEBUG] _base_client.py:1525 - Sending HTTP Request: POST http://localhost:1234/v1/chat/completions
2025-12-25 15:44:16,232 [DEBUG] _trace.py:87 - connect_tcp.started host='localhost' port=1234 local_address=None timeout=None socket_options=None
2025-12-25 15:44:16,233 [DEBUG] _trace.py:87 - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x111e40830>
2025-12-25 15:44:16,233 [DEBUG] _trace.py:87 - send_request_headers.started request=<Request [b'POST']>
2025-12-25 15:44:16,234 [DEBUG] _trace.py:87 - send_request_headers.complete
2025-12-25 15:44:16,234 [DEBUG] _trace.py:87 - send_request_body.started request=<Request [b'POST']>
2025-12-25 15:44:16,234 [DEBUG] _trace.py:87 - send_request_body.complete
2025-12-25 15:44:16,234 [DEBUG] _trace.py:87 - receive_response_headers.started request=<Request [b'POST']>
2025-12-25 15:44:16,235 [DEBUG] _trace.py:87 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'X-Powered-By', b'Express'), (b'Content-Type', b'text/event-stream'), (b'Cache-Control', b'no-cache'), (b'Connection', b'keep-alive'), (b'Date', b'Thu, 25 Dec 2025 12:44:16 GMT'), (b'Transfer-Encoding', b'chunked')])
2025-12-25 15:44:16,236 [INFO] _client.py:1740 - HTTP Request: POST http://localhost:1234/v1/chat/completions "HTTP/1.1 200 OK"
2025-12-25 15:44:16,236 [DEBUG] _base_client.py:1563 - HTTP Response: POST http://localhost:1234/v1/chat/completions "200 OK" Headers({'x-powered-by': 'Express', 'content-type': 'text/event-stream', 'cache-control': 'no-cache', 'connection': 'keep-alive', 'date': 'Thu, 25 Dec 2025 12:44:16 GMT', 'transfer-encoding': 'chunked'})
2025-12-25 15:44:16,236 [DEBUG] _base_client.py:1571 - request_id: None
2025-12-25 15:44:16,236 [DEBUG] _trace.py:87 - receive_response_body.started request=<Request [b'POST']>
2025-12-25 15:44:17,233 [DEBUG] _trace.py:87 - response_closed.started
2025-12-25 15:44:17,233 [DEBUG] _trace.py:87 - response_closed.complete
2025-12-25 15:44:17,236 [DEBUG] _trace.py:87 - receive_response_body.failed exception=GeneratorExit()
2025-12-25 17:42:09,664 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
2025-12-25 23:28:16,837 [DEBUG] selector_events.py:64 - Using selector: KqueueSelector
2025-12-25 23:28:16,837 [INFO] main.py:10 - Started
2025-12-25 23:28:16,837 [INFO] mcp_server.py:146 - Starting MCP Database Server...
2025-12-25 23:28:16,837 [DEBUG] server.py:157 - Initializing server 'DBMCPServer üöÄ'
2025-12-25 23:28:16,837 [DEBUG] server.py:414 - Registering handler for ListToolsRequest
2025-12-25 23:28:16,838 [DEBUG] server.py:281 - Registering handler for ListResourcesRequest
2025-12-25 23:28:16,838 [DEBUG] server.py:301 - Registering handler for ListResourceTemplatesRequest
2025-12-25 23:28:16,838 [DEBUG] server.py:243 - Registering handler for PromptListRequest
2025-12-25 23:28:16,838 [DEBUG] server.py:486 - Registering handler for CallToolRequest
2025-12-25 23:28:16,838 [DEBUG] server.py:316 - Registering handler for ReadResourceRequest
2025-12-25 23:28:16,838 [DEBUG] server.py:265 - Registering handler for GetPromptRequest
2025-12-25 23:28:16,846 [INFO] factory.py:76 - All fetchers initialized
2025-12-25 23:28:16,866 [INFO] metadata_connection.py:116 - DuckDB schema initialized.
2025-12-25 23:28:16,880 [INFO] postgresql_manager.py:62 - ‚úÖ DB Manager initialized. Active pools: 0
2025-12-25 23:28:16,885 [INFO] base.py:214 - Scheduler started
2025-12-25 23:28:16,885 [INFO] metadata_scheduler_manager.py:181 - Scheduler started
2025-12-25 23:28:16,915 [DEBUG] base.py:1151 - Looking for jobs to run
2025-12-25 23:28:16,915 [DEBUG] base.py:1252 - No jobs; waiting until a job is added
2025-12-25 23:28:16,915 [INFO] streamable_http_manager.py:110 - StreamableHTTP session manager started
2025-12-25 23:31:03,175 [INFO] streamable_http_manager.py:114 - StreamableHTTP session manager shutting down
